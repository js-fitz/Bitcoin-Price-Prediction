{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import threading\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2020 = pd.read_csv('./Reddit_data/2020/final_2020v2.csv')\n",
    "df_2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stage 1 source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_df = df_2020.groupby('Date').agg('sum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_df['Date'] = date_df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_list2020 = []\n",
    "for result in perdelta(date(2020, 1, 1), date(2020, 3, 8), timedelta(days=1)):\n",
    "    date_list2020.append(str(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(date_df)):\n",
    "    for j in range(len(date_list2020)-1):\n",
    "        if date_df['Date'][i] == date_list2020[j]:\n",
    "            date_list2020.remove(date_list2020[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_list2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UTC to Datetime Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def datetime(df):\n",
    "    df['Date'] = ''\n",
    "    \n",
    "    for i in range (0,len(df)):\n",
    "        df['Date'][i] = time.strftime('%Y-%m-%d', time.localtime(df['created_utc'][i]))\n",
    "    \n",
    "    df.drop_duplicates(subset =\"created_utc\", inplace = True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing Dates Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date, datetime, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/questions/10688006/generate-a-list-of-datetimes-between-an-interval\n",
    "#Credit to Martjin Peters\n",
    "\n",
    "def perdelta(start, end, delta):\n",
    "    curr = start\n",
    "    while curr < end:\n",
    "        yield curr\n",
    "        curr += delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def missingdates(df):\n",
    "    dates = []\n",
    "    \n",
    "    for result in perdelta(date(2019, 1, 1), date(2020, 1, 1), timedelta(days=1)):\n",
    "        dates.append(str(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_list2019 = []\n",
    "for result in perdelta(date(2019, 1, 1), date(2020, 1, 1), timedelta(days=1)):\n",
    "    date_list2019.append(str(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_df = df2019.groupby('Date').agg('sum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_df['date'] = date_df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#date_df.to_csv('Stage1_final_2019.csv', index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(date_df)):\n",
    "    for j in range(len(date_list2019)-1):\n",
    "        if date_df['date'][i] == date_list2019[j]:\n",
    "            date_list2019.remove(date_list2019[j])\n",
    "#date_df['date'][0] == date_list2019[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_list2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing = [1546318801, 1546405201, 1546491601, 1547010001, 1547096401, 1547701201, 1548219601, 1548306001, 1548910801, 1549515601, 1550120401, 1550725201, 1551330001, 1551934801, 1552536001, 1554350401, 1554955201, 1561608001, 1577768401]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = pd.read_csv('./Reddit_data/Combined_Yearly_Data/final_2018.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_csv('./Reddit_data/Combined_Yearly_Data/final_2018.csv')\n",
    "df2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['Date'] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range (0,len(df2)):\n",
    "    df2['Date'][i] = time.strftime('%Y-%m-%d', time.localtime(df2['created_utc'][i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.drop_duplicates(subset =\"created_utc\", inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.drop(columns = ['Unnamed: 0'], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.to_csv('./Reddit_data/Combined_Yearly_Data/stage1_final_2018.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_df18 = df2.groupby('Date').agg('sum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_df18['date'] = date_df18.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_list2018 = []\n",
    "for result in perdelta(date(2018, 1, 1), date(2019, 1, 1), timedelta(days=1)):\n",
    "    date_list2018.append(str(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(date_df18)):\n",
    "    for j in range(len(date_list2018)-1):\n",
    "        if date_df18['date'][i] == date_list2018[j]:\n",
    "            date_list2018.remove(date_list2018[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_list2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.pd.read_csv('./Reddit_data/Combined_Yearly_Data/stage1_final_2018.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = pd.read_csv('./Reddit_data/Combined_Yearly_Data/stage1_fulldates_2017.csv')\n",
    "df3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3['Date'] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range (0,len(df3)):\n",
    "    df3['Date'][i] = time.strftime('%Y-%m-%d', time.localtime(df3['created_utc'][i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.drop_duplicates(subset =\"created_utc\", inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.drop(columns = ['Unnamed: 0'], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_df17 = df3.groupby('Date').agg('sum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_list2017 = []\n",
    "for result in perdelta(date(2017, 1, 1), date(2018, 1, 1), timedelta(days=1)):\n",
    "    date_list2017.append(str(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_df17['date'] = date_df17.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(date_df17)):\n",
    "    for j in range(len(date_list2017)-1):\n",
    "        if date_df17['date'][i] == date_list2017[j]:\n",
    "            date_list2017.remove(date_list2017[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_list2017"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2016"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4 = pd.read_csv('./Reddit_data/Combined_Yearly_Data/final_2016.csv')\n",
    "df4.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4['Date'] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range (0,len(df4)):\n",
    "    df4['Date'][i] = time.strftime('%Y-%m-%d', time.localtime(df4['created_utc'][i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4.drop(columns = ['Unnamed: 0'], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4.drop_duplicates(subset =\"created_utc\", inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4.to_csv('./Reddit_data/Combined_Yearly_Data/stage1_final_2016.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add self text with title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, len(bit['title'])):\n",
    "    if pd.isnull(bit['selftext'][i]) == True:\n",
    "        pass\n",
    "    elif bit['selftext'][i] == '[removed]':\n",
    "        pass\n",
    "    else:\n",
    "        bit['title'][i] = bit['title'][i] + bit['selftext'][i]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
